[{"path":"/articles/breakpt-ex.html","id":"change-points-in-a-time-course","dir":"Articles","previous_headings":"","what":"Change points in a time course","title":"Breakpoint example","text":"order use Sys.sleep() actual computations, chose toy problem inference change points time course repeated observations measurement. First, start simulating data increasing integer x coordinate (time point) continuous y coordinate (observation value). use mcp package detect point x value y changes abruptly. package able infer many different kinds change points. , make use changing intercepts constant, normally distributed noise. package built rjags, uses Gibbs sampling Markov-Chain Monte Carlo (MCMC). computations usually computationally expensive, individual sampling chains independent . makes kind problem amenable extensive parallelization. Fortunately, rjags package already provided module, can load using following command: running , available within R via library(rjags). still need install.packages(\"mcp\"), require external tools work without issues. Note system-wide R package library writable individual users, use user library home directory (R prompt ).","code":"module load rjags/4-10-R-4.1.2"},{"path":"/articles/breakpt-ex.html","id":"creating-a-function-to-simulate-data","dir":"Articles","previous_headings":"","what":"Creating a function to simulate data","title":"Breakpoint example","text":"can use following function simulate data: randomly generate one sample x y coordinates. resulting data look something like :  Sulis, plot saved Rplots.pdf.","code":"# simulate one data set with random break points and y coordinates simulate_data = function(len=1000, bps=c(0,1,2)) {     sim = data.frame(x=seq_len(len), y=rnorm(len))     for (bp in seq_len(sample(bps, 1))) {         loc = sample(seq_len(len), 1)         sim$y[loc:len] = sim$y[loc:len] + rnorm(1)     }     sim } sim = simulate_data(bps=1) plot(sim) # dev.off()  # close the device to complete the plot"},{"path":"/articles/breakpt-ex.html","id":"running-break-point-inference","dir":"Articles","previous_headings":"","what":"Running break point inference","title":"Breakpoint example","text":"can now use simulated data starting point infer breakpoints simulated. Using mcp package, run three different models: One segment, breakpoints Two segments, one breakpoint Three segments, two breakpoints compare models terms well fit data, return model fits best: resulting model look something like :  , see x y coordinates observations , addition see traces intercepts (grey lines) probability density break point (blue line close bottom plot).","code":"library(mcp)  run_mcp = function(sim) {     # one unbroken segment, 2 segments with one break point, 3 segments 2 bps     mods = list(list(y ~ 1),                 list(y ~ 1, ~ 1),                 list(y ~ 1, ~ 1, ~ 1))      # fit all three models, select the best using leave-one-out     fits = lapply(mods, function(m) mcp::mcp(m, data=sim, par_x=\"x\"))     compare = as.data.frame(loo::loo_compare(lapply(fits, mcp::loo)))     best = as.integer(sub(\"model\", \"\", rownames(compare)))[1]     fits[[best]] } mod = run_mcp(sim) #> Compiling model graph #>    Resolving undeclared variables #>    Allocating nodes #> Graph information: #>    Observed stochastic nodes: 1000 #>    Unobserved stochastic nodes: 2 #>    Total graph size: 5019 #>  #> Initializing model #> Finished sampling in 18.7 seconds #> Compiling model graph #>    Resolving undeclared variables #>    Allocating nodes #> Graph information: #>    Observed stochastic nodes: 1000 #>    Unobserved stochastic nodes: 4 #>    Total graph size: 12020 #>  #> Initializing model #> Finished sampling in 81.5 seconds #> Compiling model graph #>    Resolving undeclared variables #>    Allocating nodes #> Graph information: #>    Observed stochastic nodes: 1000 #>    Unobserved stochastic nodes: 6 #>    Total graph size: 17028 #>  #> Initializing model #> Finished sampling in 152 seconds #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.  #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. plot(mod) # dev.off()  # close the device to complete the plot"},{"path":"/articles/breakpt-ex.html","id":"exercise","dir":"Articles","previous_headings":"Running break point inference","what":"Exercise","title":"Breakpoint example","text":"Use editor create break point simulation script (hint: can use Esc+:set paste nvim copy-paste text without automatic indentation :set nopaste return) Start interactive job 1 task 5 cores Simulate data one two break points Run mcp inference script try estimate breakpoints data. match parameters simulation? (hint: can use plot function mcp model object; create Rplots.pdf ’ll need dev.() copy local machine) options see make code run faster? (hint: mcp-provided parallelism sequential steps inference code) much runtime can save running computations parallel? Start interactive job using: nvim create file compute_breakpt.r: also see run_mcp_fast 9 times fast, gets slowed (1) higher run times complex models, (2) 5 cores requested. general, want avoid “oversubscribing” cores requested ultimately run slower use request.","code":"srun --account su105 --ntasks=1 --cpus-per-task=5 --time 6:00:00 --pty $SHELL library(mcp) simulate_data = function(len=1000, bps=c(0,1,2)) {     sim = data.frame(x=seq_len(len), y=rnorm(len))     for (bp in seq_len(sample(bps, 1))) {         loc = sample(seq_len(len), 1)         sim$y[loc:len] = sim$y[loc:len] + rnorm(1)     }     sim }  run_mcp = function(sim) {     # one unbroken segment, 2 segments with one break point, 3 segments 2 bps     mods = list(list(y ~ 1),                 list(y ~ 1, ~ 1),                 list(y ~ 1, ~ 1, ~ 1))      # fit all three models, select the best using leave-one-out     fits = lapply(mods, function(m) mcp::mcp(m, data=sim, par_x=\"x\"))     compare = as.data.frame(loo::loo_compare(lapply(fits, mcp::loo)))     best = as.integer(sub(\"model\", \"\", rownames(compare)))[1]     fits[[best]] }  run_mcp_fast = function(sim) {     # one unbroken segment, 2 segments with one break point, 3 segments 2 bps     mods = list(list(y ~ 1),                 list(y ~ 1, ~ 1),                 list(y ~ 1, ~ 1, ~ 1))      # fit all three models, select the best using leave-one-out     fits = parallel::mclapply(mods, function(m) mcp::mcp(m, data=sim, par_x=\"x\", cores=3L)) #          ^^^^^^^^^^^^^^^^^^                                                    ^^^^^^^^     compare = as.data.frame(loo::loo_compare(parallel::mclapply(fits, mcp::loo))) #                                            ^^^^^^^^^^^^^^^^^^     best = as.integer(sub(\"model\", \"\", rownames(compare)))[1]     fits[[best]] }  one_bp = simulate_data(bps=1) two_bp = simulate_data(bps=2)  system.time({ r1 = run_mcp(one_bp) }) system.time({ r2 = run_mcp_fast(one_bp) })  system.time({ r3 = run_mcp(two_bp) }) system.time({ r4 = run_mcp_fast(two_bp) })  pdf(\"breakpts.pdf\") plot(r1) plot(r2) plot(r3) plot(r4) dev.off()"},{"path":"/articles/breakpt-ex.html","id":"bigger-data-sets","dir":"Articles","previous_headings":"","what":"Bigger data sets","title":"Breakpoint example","text":"strategy reserving node many CPUs works well extent want process computations parallel CPUs given node (modern nodes often 128 cores/threads). , want simulate big computational task, use many resources limit overall amount.","code":""},{"path":"/articles/breakpt-ex.html","id":"exercise-1","dir":"Articles","previous_headings":"Bigger data sets","what":"Exercise","title":"Breakpoint example","text":"Simulate 10 breakpoint data sets using function , save resulting list .rds object (using saveRDS) Write submission script 10 tasks load object, subset current task index, save resulting model .rds model plot .pdf (hint: automatic environment variables available Slurm run, SLURM_PROCID) sim_breakpt.r submit_breakpt.sh compute_breakpt_manual.r","code":"simulate_data = function(len=1000, bps=c(0,1,2)) {     sim = data.frame(x=seq_len(len), y=rnorm(len))     for (bp in seq_len(sample(bps, 1))) {         loc = sample(seq_len(len), 1)         sim$y[loc:len] = sim$y[loc:len] + rnorm(1)     }     sim }  sims = replicate(10, simulate_data(), simplify=FALSE) saveRDS(sims, file=\"sim_breakpt.rds\") #!/bin/sh #SBATCH --account su105 #SBATCH --partition compute #SBATCH --ntasks 10 #SBATCH --cpus-per-task 1 #SBATCH --mem 2048M #SBATCH --time 8:00:00  srun Rscript compute_breakpt_manual.r library(mcp)  run_mcp = function(sim) {     # one unbroken segment, 2 segments with one break point, 3 segments 2 bps     mods = list(list(y ~ 1),                 list(y ~ 1, ~ 1),                 list(y ~ 1, ~ 1, ~ 1))      # fit all three models, select the best using leave-one-out     fits = lapply(mods, function(m) mcp::mcp(m, data=sim, par_x=\"x\"))     compare = as.data.frame(loo::loo_compare(lapply(fits, mcp::loo)))     best = as.integer(sub(\"model\", \"\", rownames(compare)))[1]     fits[[best]] }  idx = Sys.getenv(\"SLURM_PROCID\") # zero-indexed  dset = readRDS(\"sim_breakpt.rds\") cur_dset = dset[[as.integer(idx) + 1]] # one-indexed  res = run_mcp(cur_dset) pdf(paste0(\"bp\", idx, \".pdf\")) dev.off() saveRDS(res, file=paste0(\"bp\", idx, \".rds\"))"},{"path":"/articles/breakpt-ex.html","id":"hpc-specific-packages","dir":"Articles","previous_headings":"","what":"HPC-specific packages","title":"Breakpoint example","text":"also multiple packages available make use HPC resources within R. say, R submit job multiple jobs, retrieve result back session. , instance, packages BatchJobs batchtools. make use networked file system write call arguments file, retrieved job executed. packages robust small numbers jobs, however, put substantial strain file system high number function calls. Instead, introduce two packages make little use shared file system, slurmR clustermq. (Note author latter, ’s bit conflict interest .) also Rmpi package, provides R cluster object e.g. parLapply multiple instances communicating different nodes. example Sulis docs, however, requires additional setup job submission script.","code":""},{"path":"/articles/breakpt-ex.html","id":"slurmr","dir":"Articles","previous_headings":"HPC-specific packages","what":"slurmR","title":"Breakpoint example","text":"slurmR package lightweight (dependency-free) R package allows users interact scheduler. can used create cluster object Slurm tasks analogous parallel::makePSOCKcluster. need separate submission script, run interactive job strain login node. command slurmR::makeSlurmCluster(ntasks), can used parLapply parSapply functions: sbatch call created via makeSlurmCluster can customized passing different parameters cluster creation function. particular, need : Include right budgeting account jobs started automatically slurmR inherit environment used interactive job. problem ’ve added module comamnds .bashrc file previous exercises. details can found Getting Started vignette.","code":"# install.packages(\"slurmR\") if you have not installed the package yet library(slurmR)  cl = makeSlurmCluster(5, account=\"su105\")  system.time({ parSapply(cl, 1:10, function(i) Sys.sleep(1)) })  stopCluster(cl)"},{"path":"/articles/breakpt-ex.html","id":"clustermq","dir":"Articles","previous_headings":"HPC-specific packages","what":"clustermq","title":"Breakpoint example","text":"clustermq package provides interface multiple HPC schedulers (including Slurm) via ZeroMQ socket library, available module assuming prerequisite modules R already loaded ; load compatible ZeroMQ module. package relies submission template, default submits Slurm jobs job array. need supply account name, policy Sulis use tasks whenever possible, modify submission template : Include right budgeting account Use srun multiple tasks per index job array actually working (yet) address points, can create new file cmq_slurm.tmpl following contents: can use newly created template : can either run lines active R session, add ~/.Rprofile set automatically time R started. make example run 10 tasks per job requested. information configure use clustermq available User Guide vignette.","code":"module load ZeroMQ/4.3.4 # install.packages(\"clustermq\") if you have not installed the package yet library(clustermq)  Q(function(i) Sys.sleep(1), i=1:10, n_jobs=5) #!/bin/sh #SBATCH --account={{ account | su105 }} #SBATCH --job-name={{ job_name }} #SBATCH --array=1-{{ n_jobs }} #SBATCH --mem-per-cpu={{ memory | 1024M }} #SBATCH --cpus-per-task={{ cores | 1 }}  module load rjags/4-10-R-4.1.2 # this we will need for our example  CMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker(\"{{ master }}\")' options(clustermq.host = \"ib0\", # faster network         clustermq.template = \"/path/to/cmq_slurm.tmpl\")"},{"path":"/articles/breakpt-ex.html","id":"exercise-2","dir":"Articles","previous_headings":"HPC-specific packages","what":"Exercise","title":"Breakpoint example","text":"Run workers slurmR Run workers clustermq interactive job yet, start using nvim create file compute_breakpt_slurmR.r: interactive job yet, start using nvim create file compute_breakpt_clustermq.r:","code":"srun --account su105 --ntasks=1 --cpus-per-task=1 --time 6:00:00 --pty $SHELL library(slurmR)  run_mcp = function(sim) {     library(mcp)     # one unbroken segment, 2 segments with one break point, 3 segments 2 bps     mods = list(list(y ~ 1),                 list(y ~ 1, ~ 1),                 list(y ~ 1, ~ 1, ~ 1))      # fit all three models, select the best using leave-one-out     fits = lapply(mods, function(m) mcp::mcp(m, data=sim, par_x=\"x\"))     compare = as.data.frame(loo::loo_compare(lapply(fits, mcp::loo)))     best = as.integer(sub(\"model\", \"\", rownames(compare)))[1]     fits[[best]] }  dset = readRDS(\"sim_breakpt.rds\")  cl = makeSlurmCluster(10, account=\"su105\") res = parLapply(cl, dset, run_mcp) stopCluster(cl)  saveRDS(res, file=\"breakpt_slurmR.rds\") srun --account su105 --ntasks=1 --cpus-per-task=1 --time 6:00:00 --pty $SHELL library(clustermq)  run_mcp = function(sim) {     # one unbroken segment, 2 segments with one break point, 3 segments 2 bps     mods = list(list(y ~ 1),                 list(y ~ 1, ~ 1),                 list(y ~ 1, ~ 1, ~ 1))      # fit all three models, select the best using leave-one-out     fits = lapply(mods, function(m) mcp::mcp(m, data=sim, par_x=\"x\"))     compare = as.data.frame(loo::loo_compare(lapply(fits, mcp::loo)))     best = as.integer(sub(\"model\", \"\", rownames(compare)))[1]     fits[[best]] }  dset = readRDS(\"sim_breakpt.rds\")  res = Q(run_mcp, sim=dset, pkgs=\"mcp\", n_jobs=10)  saveRDS(res, file=\"breakpt_clustermq.rds\")"},{"path":"/articles/neovim-ide.html","id":"making-r-and-python-available-by-default","dir":"Articles","previous_headings":"","what":"Making R and Python available by default","title":"Neovim as IDE","text":"certain tools expect use often available load via modules. instance, likely frequently make use R, python, compiler (GCC), editor (Neovim). connect computing cluster, open shell, .e. command-line interface . default bash, usually available Unix-like systems. (However, longer default macOS favor zsh.) Every time start bash login node, runs configuration file, ~/.bashrc. automatically load standard tool set connect, can add module load commands file: edited file, can either close connection reconnect, type source ~/.bashrc reload . Note, however, loading modules result bit slowdown establish connection login node. may want normal use, simplify working rest course materials. can also change things, like adding colors prompt display current directory instead just folder name: add useful aliases make listing files navigating different directories bit easier:","code":"# User specific aliases and functions  if ! command -v R >/dev/null 2>&1; then     module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2     module load GCCcore/11.2.0 Python/3.9.6 CMake/3.21.1     module load Neovim/0.6.1 fi export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h\\[\\e[1;31m\\](sulis)\\[\\e[0m\\]:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" alias l='ls -lh' alias ll='ls -lah' alias .='pwd' alias cd..='cd ..' alias ..='cd ..' alias ...=' cd ../..' alias ....='cd ../../..' alias .....='cd ../../../..'"},{"path":[]},{"path":"/articles/neovim-ide.html","id":"configuration","dir":"Articles","previous_headings":"Editing R scripts using Neovim","what":"Configuration","title":"Neovim as IDE","text":", edit file ~/.config/nvim/init.vim (create directory first using mkdir -p) include:","code":"set expandtab       \" tab as spaces (optional) set shiftwidth=4    \" shift by 4 spaces set tabstop=4       \" tab by 4 spaces set smarttab        \" tab to same positions \"set autoindent      \" automatic indentation (good for editing, less for pasting) set mouse=a         \" be able to use mouse clicks  set nobackup        \" use an undo file instead of backup files set undofile set undodir=~/.config/nvim/undo  tnoremap <Esc> <C-\\><C-n>   \" fix Esc in terminal  \" Uncomment this when adding the plugins\\ # Note: this has to be listed before using the plugins (below) \"call plug#begin() \"    Plug 'sheerun/vim-wombat-scheme' \"    Plug 'jalvesaq/Nvim-R' \"    Plug 'ycm-core/YouCompleteMe', { 'do': './install.py' } \"    Plug 'ntpeters/vim-better-whitespace' \"call plug#end()  \"colorscheme wombat \" uncomment this after installing the plugin  \" uncomment after installing Nvim-R, use <Space> to send commands to R \"let g:R_assign = 0 \" do (not) map _ to <- for assignment \"vmap <Space> <Plug>RDSendSelection \"nmap <Space> <Plug>RDSendLine \" ^^^^^^ make sure there are are no extra spaces at the end of those lines"},{"path":"/articles/neovim-ide.html","id":"plugins","dir":"Articles","previous_headings":"Editing R scripts using Neovim","what":"Plugins","title":"Neovim as IDE","text":"(Neo)vim emacs powerful editors many useful functions text editing, also interactive development R languages. functionality provided plugins. emacs mainly ESS (emacs speaks statistics), nvim use plugin manager plug, can install using following lines code: copy file plug.vim Github ~/.local/share/nvim/site/autoload directory. Note: executing bits code internet, make sure trustworthy sources. case, comes nvim-plug Github repository, official source. add following plugins: Nvim-R YouCompleteMe Wombat color scheme Whitespace highlighter , uncomment file ~/.config/nvim/init.vim: Note need Python pynvim work: can install plugins Neovim using Esc :","code":"curl -fLo ~/.local/share/nvim/site/autoload/plug.vim --create-dirs \\     https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim call plug#begin()     Plug 'sheerun/vim-wombat-scheme'     Plug 'jalvesaq/Nvim-R'     Plug 'ycm-core/YouCompleteMe', { 'do': './install.py' }     Plug 'ntpeters/vim-better-whitespace' call plug#end() module load GCCcore/11.2.0 Python/3.9.6 CMake/3.21.1 pip install --user pynvim :PlugInstall"},{"path":"/articles/neovim-ide.html","id":"using-r-within-nvim","dir":"Articles","previous_headings":"Editing R scripts using Neovim","what":"Using R within nvim","title":"Neovim as IDE","text":"Note: avoid load login node, start nvim interactive job plan running computations R. can uncomment lines previously commented ~/.config/nvim/init.vim. Now, using Nvim-R plugin, can access R directly nvim. , can open .r file, type \\rf. can now edit file editor , addition send lines R via Space, terminal directly selecting mouse (Ctrl+w followed arrow keys).","code":""},{"path":"/articles/neovim-ide.html","id":"persistent-sessions-with-tmux","dir":"Articles","previous_headings":"","what":"Persistent sessions with tmux","title":"Neovim as IDE","text":"far, created new terminal session time connected login node. cases may instead want continue work left previously. tools create “persistent” session login node can disconnect later connect without losing work session. well-known tool screen, recently tmux. tmux installed login node, can use box typing: first view, much changed. addition normal terminal prompt, now status line bottom screen. can now perform operations, like typing ls pwd. want disconnect, can use keys Ctrl+b, d type: close tmux session. can now disconnect login node, later reconnect, re-attach session typing: 0 case (automatic) name session created previously. addition, can also split terminal 2 vertical “panes” using Ctrl+b, % create new “window” using Ctrl+b, c. panes, windows, open applications (e.g. text editor) waiting us time detach re-attach session. Ctrl+b common prefix, typed tmux command.","code":"tmux new tmux detach tmux attach -t 0"},{"path":"/articles/neovim-ide.html","id":"a-ssht-alias-to-automatically-connect-to-tmux","dir":"Articles","previous_headings":"Persistent sessions with tmux","what":"A ssht alias to automatically connect to tmux","title":"Neovim as IDE","text":"can define following function automatically connect (create) tmux session type ssht sulis instead ssh sulis. using bash shell locally well, can add lines local ~/.bashrc:","code":"ssht() {     ssh -t \"$1\" 'tmux -2 -u attach || tmux -2 -u new' }"},{"path":"/articles/neovim-ide.html","id":"making-tmux-easier-to-use","dir":"Articles","previous_headings":"Persistent sessions with tmux","what":"Making tmux easier to use","title":"Neovim as IDE","text":"lot different commands remember, especially first starting use tmux. can add configuration options ~/.tmux.conf make using bit intuitive: editing config file, reload using, Sulis shell:","code":"## enable Ctrl+a as prefix for people used to \"screen\" #set -g prefix C-a #bind C-a send-prefix #unbind C-b  # enable mouse clicks to select windows and panes set -g mouse on  # easy-to-remember split pane commands bind | split-window -h -c \"#{pane_current_path}\" bind - split-window -v -c \"#{pane_current_path}\"  # swap current pane with the next/previous one bind > swap-pane -D bind < swap-pane -U tmux source-file ~/.tmux.conf"},{"path":"/articles/quickstart.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Quick Start","text":"Working High-Performance Computing (HPC) facilities, primarily interface systems via command-line shell. expect people course wide range expertise different starting points concerning use command-line, make easier challenging participants, depending coming . Similarly, expect participants start different operating systems, e.g. Windows also macOS different Linux flavors. order keep course consistent, recommend Windows users work materials using Windows Subsystem Linux (WSL 2.0). available --date Windows 10 later, installation instructions available . work, can also use MobaXterm. Likely, macOS Linux users already familiar terminal. Note: WSL, home directory (every time refer paths starting ~) likely folder like /mnt/c/Users/<UserName>. Note: using MobaXterm, sure directory running local (networked file system) write permissions . Note: using WSL, ’ll want enable Ctrl+Shift+V copy-paste. Otherwise still able use Shift+Right Mouse.","code":""},{"path":"/articles/quickstart.html","id":"connecting-to-the-computing-cluster","dir":"Articles","previous_headings":"","what":"Connecting to the computing cluster","title":"Quick Start","text":"Command-line connections HPC established using Secure Shell tool (SSH). terminal, can connect Sulis using following line: Since bit tedious, can outsource information ~/.ssh/config file following contents: set like , establishing connection simple typing: come handy connecting HPC , also copying files local machine HPC. Note, however, cases still need decrypt private key file passphrase (, least macOS Linux, can automated using OS keychain) 2FA token day. Login Sulis documented detail: https://sulis-hpc.github.io/gettingstarted/connecting/","code":"ssh -i <your keyfile> <user>@login.sulis.ac.uk Host sulis     Hostname login.sulis.ac.uk     User <username> #    ProxyJump <proxy>   # when connecting via the host entry \"<proxy>\"     IdentityFile ~/.ssh/<sulis_rsa> ssh sulis"},{"path":"/articles/quickstart.html","id":"setting-up-r","dir":"Articles","previous_headings":"","what":"Setting up R","title":"Quick Start","text":"","code":"which R # /usr/bin/which: no R in [...] module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 which R # /sulis/easybuild/software/R/4.1.2-foss-2021b/bin/R"},{"path":"/articles/quickstart.html","id":"running-an-interactive-job","dir":"Articles","previous_headings":"","what":"Running an interactive job","title":"Quick Start","text":"simplest way request interactive job use Slurm’s srun command specify want run shell (specified $SHELL environment variable) connected terminal input output (--pty). addition, need specify account requested resources budgeted (--account) can running: see command prompt changes user@login user$nodeXX, means now connected compute node instead login node. , allowed run heavy computations within resource constraints specified. First, let’s get overview processes already running node. can running resource monitor htop: overview, can see many cores compute node , many processes running, much memory used. Depending much resources requested (overall load), see least resource allocation still free. , however, need stay within allocation (overall amount available resources), otherwise processes terminated automatically. loaded R module beforehand, ’ll see $PATH (environment variable shell looks executables) still set include R. can check asking R path: can run R via command-line, can local machines well. Running R give us R command prompt: can use R use R shell e.g. RStudio well: done, can exit R typing quit(save=\"\"). Note exiting R interactive job still running. can exit job shell typing exit Ctrl+d, unless started interactive job srun --pty R instead $SHELL. Note exiting job sometimes display message Exited exit code 127. can safely ignored.","code":"srun --account su105 --pty $SHELL htop # quit by typing 'q' which R #>  #> R version 4.1.3 (2022-03-10) -- \"One Push-Up\" #> Copyright (C) 2022 The R Foundation for Statistical Computing #> Platform: x86_64-pc-linux-gnu (64-bit) #>  #> R is free software and comes with ABSOLUTELY NO WARRANTY. #> You are welcome to redistribute it under certain conditions. #> Type 'license()' or 'licence()' for distribution details. #>  #>   Natural language support but running in an English locale #>  #> R is a collaborative project with many contributors. #> Type 'contributors()' for more information and #> 'citation()' on how to cite R or R packages in publications. #>  #> Type 'demo()' for some demos, 'help()' for on-line help, or #> 'help.start()' for an HTML browser interface to help. #> Type 'q()' to quit R. #>  #> >  #> > x = 5 y = 3 x * y #> [1] 15"},{"path":"/articles/quickstart.html","id":"copying-files","dir":"Articles","previous_headings":"","what":"Copying files","title":"Quick Start","text":"different options getting files compute cluster. One option edit files locally, copy SSH. , instance, one local file test.txt copy : specified sulis ~/.ssh/config , otherwise may need specify user name, key file, host manually (look differently using graphical SSH client). : used scp command know one remote end, .e. use following copy file remote local end (run machine, sulis): , scp command looks test.txt home directory (~; default directory specified) copy current directory (.). want copy directories need use recursive copying, .e. scp -r. Another, maybe better alternative rsync command. keep timestamps intact, can used copy files updated timestamps (-u) compared local files (-v print files copying): Note: copy commands, Sulis run local machine.","code":"scp test.txt sulis: scp sulis:test.txt . # '.' means current directory (`pwd`) rsync -uvr sulis:test.txt ."},{"path":"/articles/quickstart.html","id":"editing-files","dir":"Articles","previous_headings":"","what":"Editing files","title":"Quick Start","text":"either making small changes iterative work, often convenient edit files directly computing cluster instead editing locally copy . multiple text-based editors work terminal, nano, emacs vim. nano minimalist editor without special features, often recommended users new terminal. problem get quickly stuck local optimum, can make simple changes file, never get features syntax highlighting. two editors hand either can extended /every feature imaginable. course, show basic features nvim (neovim, modern implementation vim) instead. already user emacs, please feel free use editor instead. edit simple text file, can run: see console gets cleared, shown contents empty file instead. Try typing couple file: see characters show , editor well. Notice, however, first typed show , subsequent . editor “normal” “edit” mode. typing first , switched former latter. can now use Esc switch back edit normal mode. normal mode, can type :w Enter write file, :wq write file quit editor. :q! exit without saving changes. need know make vim useful nano. However, now instance edit .r file also get syntax highlighting. feature alone makes worth use nvim instead nano. can explore features running vimtutor command-line, interactive tool familiarize use effectively.","code":"# first run 'module load Neovim/0.6.1' if in an interactive job nvim myfile.txt aaaa"},{"path":"/articles/quickstart.html","id":"compute-resources","dir":"Articles","previous_headings":"","what":"Compute resources","title":"Quick Start","text":"now, submitted job specifying minimum required parameters relying defaults others. instance, specific partition, one several job queues can submit jobs . get overview available, can use sinfo command: , see different partitions listed number nodes associated , including walltime (maximum amount time job can request) queue. see one queue marked *. denotes default queue, using specifying particular queue via --partition parameter. One argument specify explain detail account. specifies connection user name collection resources available can use, subtracted budget. can check accounts user access typing: likely belong one account, project, time (one created course).","code":"sinfo sacctmgr show associations where user=<your user name>"},{"path":"/articles/quickstart.html","id":"job-submission-scripts","dir":"Articles","previous_headings":"","what":"Job submission scripts","title":"Quick Start","text":"Usually, want run complex computations can specified single srun. running multiple commands multiple hosts, often better specify resource requirements exact commands using job submission script. may look like following: can submit script saving script file running sbatch <script>. tell us something like: Submitted batch job 1290046 number identifier job (different multiple runs). started , can check jobs user running typing: list job ID, show currently running, node job running . can also get detailed information job using scontrol: information available job running (short) time finished. can now also see resources explicitly request, e.g. time limit 1 hour, used compute partition, able use 1-4 Gb memory (may depend node). job started, create output file called slurm-xxxxxx.(xxxxxx job id) standard output command (standard output otherwise printed console). finished, contain output uname -n, name node command run . went well, contain nodeXX login, means run one compute nodes. ’s quite lines , let’s break : #!/bin/sh called shebang specifies application used run script, required sbatch (otherwise refuse submit job) #SBATCH --account needed budget resources correctly #SBATCH --partition time specify compute partition explicitly #SBATCH --ntasks lists tasks (computations) job contains #SBATCH --cpus-per-task specifies numbers CPUs per task #SBATCH --mem specifies amount memory requested; total, options include --mem-per-task, --mem-per-cpu, --mem-per-gpu. Memory multipliers K, M G supported (kilobytes, megabytes gigabytes, respectively). #SBATCH --time maximum amount time job terminated (dd-hh:mm:ss) #SBATCH commands need directly following shebang, otherwise ignored srun specifies command run; required running individual computations, helps set parallel helpers, running call per task, e.g. setting MPI used","code":"#!/bin/sh #SBATCH --account su105 #SBATCH --partition compute #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH --mem 1024M #SBATCH --time 8:00:00  srun uname -n squeue -u <username> scontrol show jobid <jobid>"},{"path":"/articles/quickstart.html","id":"exercise","dir":"Articles","previous_headings":"Job submission scripts","what":"Exercise","title":"Quick Start","text":"Create batch submission script like one using command-line editor cluster Submit using sbatch <script>, without srun. changes? happens change ntasks parameter 2, without srun command?","code":""},{"path":"/articles/quickstart.html","id":"r-on-hpc","dir":"Articles","previous_headings":"","what":"R on HPC","title":"Quick Start","text":"R’s best developed parallel capabilities running computations multiple cores. , briefly outline approaches commonly used. simplicity, let’s consider simple R function sleeps couple seconds: need call function 10 times, use something like lapply: , unsurprisingly, take 10 times long individual call fsleep(). many use cases, makes sense run computations (done reality instead just sleeping) parallel. probably integrated solution parallel package, one core packages distributed standard R installation default. However, also possibilities, e.g. foreach package enables parallel processing using %dopar% command, future package using plan(multisession).","code":"fsleep = function(i) {     print(\"starting \", i)     Sys.sleep(1)     print(\"done!\") }  fsleep(1) #> [1] \"starting \" #> [1] \"done!\" system.time({ lapply(1:10, fsleep) }) #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #>    user  system elapsed  #>   0.005   0.000  10.016 system.time({ parallel::mclapply(1:10, fsleep) }) #>    user  system elapsed  #>   0.004   0.009   5.020"},{"path":"/articles/quickstart.html","id":"exercise-1","dir":"Articles","previous_headings":"R on HPC","what":"Exercise","title":"Quick Start","text":"Create sleep.r script based fsleep function mclapply calling function 10 times. Print result system.time(). Create submission script request 1 task 5 cores (hint: see documentation ) Submit script job. run successfully? long take according job log file? runtime make sense? Submit script using parallel foreach %dopar% loop parallel::makePSOCKcluster. run? results make sense? newer approach aims providing common interface many parallel backends future package. Can run example using plan(multisession) future.apply? start , created two files, sleep.r submit_sleep.sh. sleep.r submit_sleep.sh run terminal: Running script, see slurm-XXXXX.runtime 5 seconds, corresponds 2 cores used, 5 requested. parallel::mclapply uses mc.cores = getOption(\"mc.cores\", 2L), can see defaults 2. can check inspecting mclapply function: One way specify correct number cores uncomment # export line submission script. Running report 2 seconds, expected. Using foreach %dopar% parallel processing, write following script: sleep_foreach.r submit_sleep_foreach.sh run terminal: get following warning message: Warning message: _executing %dopar% sequentially: parallel backend registered register backend, can uncomment # line sleep_foreach.r. Now also finishes within expected 2 seconds instead 10. sleep_psock.r submit_sleep_psock.sh run terminal: report ran 2 seconds. sleep_future.r submit_sleep_future.sh run terminal: report runtime bit 2 seconds saw previously. reason future framework adds overhead parallel package provides.","code":"fsleep = function(i) {     print(\"starting \", i)     Sys.sleep(1)     print(\"done!\") }  system.time({ parallel::mclapply(1:10, fsleep) }) #!/bin/sh #SBATCH --account su105 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 5 #SBATCH --mem 1024M #SBATCH --time 0:10:00  # export MC_CORES=$SLURM_CPUS_ON_NODE srun Rscript sleep.r sbatch submit_sleep.sh head(parallel::mclapply) #>                                                                            #> 1 function (X, FUN, ..., mc.preschedule = TRUE, mc.set.seed = TRUE,        #> 2     mc.silent = FALSE, mc.cores = getOption(\"mc.cores\", 2L),             #> 3     mc.cleanup = TRUE, mc.allow.recursive = TRUE, affinity.list = NULL)  #> 4 {                                                                        #> 5     cores <- as.integer(mc.cores)                                        #> 6     if ((is.na(cores) || cores < 1L) && is.null(affinity.list)) getOption(\"mc.cores\") #> NULL getOption(\"mc.cores\", 2L) #> [1] 2 library(foreach) # library(doParallel) # registerDoParallel(cores=getOption(\"mc.cores\", 2L))  fsleep = function(i) {     print(\"starting \", i)     Sys.sleep(1)     print(\"done!\") }  system.time({ foreach(i=1:10) %dopar% fsleep(i) }) #!/bin/sh #SBATCH --account su105 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 5 #SBATCH --mem 1024M #SBATCH --time 0:10:00  export MC_CORES=$SLURM_CPUS_ON_NODE srun Rscript sleep_foreach.r sbatch submit_sleep_foreach.sh library(parallel) cl = makePSOCKcluster(getOption(\"mc.cores\"))  fsleep = function(i) {     print(\"starting \", i)     Sys.sleep(1)     print(\"done!\") }  system.time({ parLapply(cl, 1:10, fsleep) })  stopCluster(cl) #!/bin/sh #SBATCH --account su105 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 5 #SBATCH --mem 1024M #SBATCH --time 0:10:00  export MC_CORES=$SLURM_CPUS_ON_NODE srun Rscript sleep_psock.r sbatch submit_sleep_psock.sh library(future.apply) plan(multicore)  fsleep = function(i) {     print(\"starting \", i)     Sys.sleep(1)     print(\"done!\") }  system.time({ future_lapply(1:10, fsleep) }) #!/bin/sh #SBATCH --account su105 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 5 #SBATCH --mem 1024M #SBATCH --time 0:10:00  export MC_CORES=$SLURM_CPUS_ON_NODE srun Rscript sleep_future.r sbatch submit_sleep_future.sh"},{"path":"/articles/workflows.html","id":"from-tasks-to-workflows","dir":"Articles","previous_headings":"","what":"From tasks to workflows","title":"Workflows","text":"far, parallelized computations across either CPU cores Slurm tasks. computations wanted parallelize largely homogeneous. Real-world projects, however, involve many different processing steps, parts lend run arbitrarily parallel. section, use breakpoint example treat workflow. idea split computing breakpoints (compute_breakpt.r) plotting individual resulting model (plot.r) summary report (report.r). try run breakpoint computation plotting parallel, finish project making simple report data. also try run computations job Sulis.","code":""},{"path":"/articles/workflows.html","id":"shell-scripting","dir":"Articles","previous_headings":"","what":"Shell scripting","title":"Workflows","text":"job script multiple cores, can run multiple srun commands parallel using & sign end command (denotes script wait command done, continue) wait command stop parallel commands done. look like following: , additionally use && chain together commands: second run first one completed (plotting data makes sense generated data first place).","code":"#!/bin/sh #SBATCH commands go here  RDS=\"bp1.rds bp2.rds bp3.rds ...\"  for FILE in $RDS; do     ( srun Rscript compute_breakpt.r $FILE && Rscript plot.r $FILE ) & done  wait  srun Rscript report.r"},{"path":"/articles/workflows.html","id":"exercise","dir":"Articles","previous_headings":"Shell scripting","what":"Exercise","title":"Workflows","text":"Submit job uses shell scripting produce computation results (bp<index>.rds), plots (bp<index>.pdf) report (report.pdf)","code":""},{"path":"/articles/workflows.html","id":"gnu-parallel","dir":"Articles","previous_headings":"","what":"GNU parallel","title":"Workflows","text":"GNU parallel able distribute number script calls parallel, using available CPU cores, available tasks combination srun. extensively documented https://sulis-hpc.github.io/advanced/ensemble/gnuparallel.html.","code":""},{"path":"/articles/workflows.html","id":"exercise-1","dir":"Articles","previous_headings":"GNU parallel","what":"Exercise","title":"Workflows","text":"Submit job 2 tasks Use GNU parallel srun previous break point inference making use two tasks Add report generation script reads results files generates summary","code":""},{"path":"/articles/workflows.html","id":"gnu-make","dir":"Articles","previous_headings":"","what":"GNU make","title":"Workflows","text":"GNU make tool commonly used compile assemble software executables, schedules execution different steps within number available jobs. contrast GNU parallel, can distribute different kinds calls together common framework. example Makefile processing breakpoint example look something like : placeholder variables $^ $@ refer input (right : rule) output (left : rule), respectively. can queried using commandArgs() function R. can type make -n see commands GNU make execute without actually running .","code":"R = Rscript ALL_IDX = $(shell seq 1 10) ALL_BP = $(ALL_IDX:%=bp%.rds)  report.pdf: $(ALL_BP)     $(R) report.r $@ $^  # create a breakpoint result file bp%.rds: data.rds     $(R) compute_breakpt.r $@ $^  # convert a breakpoint result file to a plot file bp%.pdf: bp%.rds     $(R) plot.r $@ $^"},{"path":"/articles/workflows.html","id":"exercise-2","dir":"Articles","previous_headings":"GNU make","what":"Exercise","title":"Workflows","text":"Edit Makefile previously provided breakpoint scripts generate data, process data, plot data Run make command job 2 cores Replace Rscript call $(R) srun Rscript run process job 2 tasks 1 core instead","code":""},{"path":"/articles/workflows.html","id":"snakemake","dir":"Articles","previous_headings":"","what":"snakemake","title":"Workflows","text":"Snakemake Python package command-line tool lets write rules chain multiple computations together, run parallel. typical Snakefile may look something like : Snakemake looks complicated GNU make first look, features former . instance, supports multiple wildcards per file name, feature can useful complicated workflows. can type snakemake -np see commands Snakemake execute without actually running . Note want use Snakemake regularly, Slurm profile available part excercises .","code":"rule report:     input:         rscript = \"report.r\",         infiles = expand(\"{index}.rds\", index=range(10))     output:         report = \"report.pdf\"     shell:         \"Rscript {input.rscript} {output.report} {input.infiles}\"  rule compute:     input:         rscript = \"compute_breakpt.r\",         datafile = \"data.rds\"     output:         outfile = \"bp{index}.rds\"     shell:         \"Rscript {input.rscript} {input.datafile} {output.outfile}\"  rule plot:     input:         rscript = \"plot.r\",         infile = \"bp{index}.rds\"     output:         plotfile = \"bp{index}.pdf\"     shell:         \"Rscript {input.rscript} {input.infile} {output.plotfile}\""},{"path":"/articles/workflows.html","id":"exercise-3","dir":"Articles","previous_headings":"snakemake","what":"Exercise","title":"Workflows","text":"Write Snakefile corresponding R scripts rules breakpoint calculation plotting separately Run job 1 task 2 cores Change Rscript calls srun Rscript run report generation job 2 tasks 1 core ","code":""},{"path":"/articles/workflows.html","id":"targets","dir":"Articles","previous_headings":"","what":"targets","title":"Workflows","text":"R package targets provides workflow engine pure R. well documented https://books.ropensci.org/targets/.","code":""},{"path":"/articles/workflows.html","id":"exercise-4","dir":"Articles","previous_headings":"targets","what":"Exercise","title":"Workflows","text":"Write R script uses targets process breakpoint calculation, plotting, making summary report","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michael Schubert. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Schubert M (2022). Rhpc: R-hpc Course Material. R package version 0.1.0, https://mschubert.github.io/R-hpc/.","code":"@Manual{,   title = {Rhpc: R-hpc Course Material},   author = {Michael Schubert},   year = {2022},   note = {R package version 0.1.0},   url = {https://mschubert.github.io/R-hpc/}, }"},{"path":"/index.html","id":"high-performance-computing-with-the-r-programming-language","dir":"","previous_headings":"","what":"R-hpc Course Material","title":"R-hpc Course Material","text":"material developed pilot 2-day workshop University Warwick March 2022, associated Sulis tier 2 HPC service funded EPSRC grant EP/W032201/1. version material used pilot contains instructions/examples tailored Sulis HPC service. forked upstream repository Dr Michael Schubert retain Sulis specific elements.","code":""},{"path":[]},{"path":"/index.html","id":"morning-intoduction-to-hpc-and-the-sulis-tier-2-service","dir":"","previous_headings":"Day 1","what":"Morning: Intoduction to HPC and the Sulis tier 2 service","title":"R-hpc Course Material","text":"Capabilities computing cluster connect Starting basic job","code":""},{"path":"/index.html","id":"afternoon-quick-start","dir":"","previous_headings":"Day 1","what":"Afternoon: Quick Start","title":"R-hpc Course Material","text":"Copying editing files via command-line Interactive jobs, batch jobs parallel package, cluster objects, future","code":""},{"path":"/index.html","id":"afternoon-neovim-as-ide","dir":"","previous_headings":"Day 1","what":"Afternoon: Neovim as IDE","title":"R-hpc Course Material","text":"Nvim-R plugin interactively develop remote session Persistent server sessions using tmux","code":""},{"path":[]},{"path":"/index.html","id":"morning-breakpoint-example","dir":"","previous_headings":"Day 2","what":"Morning: Breakpoint example","title":"R-hpc Course Material","text":"MCMC breakpoint detection method example Tasks vs. threads bigger jobs R packages HPC use: slurmR clustermq","code":""},{"path":"/index.html","id":"afternoon-workflows","dir":"","previous_headings":"Day 2","what":"Afternoon: Workflows","title":"R-hpc Course Material","text":"GNU parallel GNU make Snakemake targets R package","code":""},{"path":"/index.html","id":"afternoon","dir":"","previous_headings":"Day 2","what":"Afternoon:","title":"R-hpc Course Material","text":"Possibility attendees work projects help instructors","code":""},{"path":[]}]
